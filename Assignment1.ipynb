{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
    "\n",
    "- solving a problem of n-grams frequencies storing for a large corpus;\n",
    "- taking into account keyboard layout and associated misspellings;\n",
    "- efficiency improvement to make the solution faster;\n",
    "- ...\n",
    "\n",
    "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
    "\n",
    "##### IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import permutations\n",
    "\n",
    "def load_ngrams(file_path):\n",
    "    ngram_dict = {}\n",
    "    with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 1:\n",
    "                freq = int(parts[0])\n",
    "                ngram = tuple(parts[1:])\n",
    "                ngram_dict[ngram] = freq\n",
    "    return ngram_dict\n",
    "\n",
    "data_files = [\"bigrams (2).txt\"]\n",
    "ngram_data = {file: load_ngrams(file) for file in data_files}\n",
    "\n",
    "bigrams = ngram_data[\"bigrams (2).txt\"]\n",
    "\n",
    "def extract_unigrams_from_ngrams(bigrams):\n",
    "    unigrams = defaultdict(int)\n",
    "    for (w1, w2), freq in bigrams.items():\n",
    "        unigrams[w1] += freq\n",
    "        unigrams[w2] += freq\n",
    "    return unigrams\n",
    "\n",
    "unigrams = extract_unigrams_from_ngrams(bigrams)\n",
    "\n",
    "def damerau_levenshtein(s1, s2):\n",
    "    d = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "    for i in range(len(s1) + 1):\n",
    "        for j in range(len(s2) + 1):\n",
    "            if i == 0:\n",
    "                d[i][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][j] = i\n",
    "            else:\n",
    "                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "                d[i][j] = min(d[i - 1][j] + 1,\n",
    "                               d[i][j - 1] + 1,\n",
    "                               d[i - 1][j - 1] + cost)\n",
    "                if (i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]):\n",
    "                    d[i][j] = min(d[i][j], d[i - 2][j - 2] + 1)\n",
    "    return d[len(s1)][len(s2)]\n",
    "\n",
    "def generate_candidates(word):\n",
    "    candidates = {word}\n",
    "    for unigram in unigrams.keys():\n",
    "        if abs(len(word) - len(unigram)) <= 2 and damerau_levenshtein(word, unigram) <= 2:\n",
    "            candidates.add(unigram)\n",
    "    return candidates if candidates else {word}\n",
    "\n",
    "class SpellCorrector:\n",
    "    def __init__(self, unigrams, bigrams, beam_width):\n",
    "        self.unigrams = unigrams\n",
    "        self.bigrams = bigrams\n",
    "        self.total_unigrams = sum(unigrams.values())\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "    def P(self, word):\n",
    "        return self.unigrams.get(word, 1) / self.total_unigrams\n",
    "\n",
    "    def P_bigram(self, w1, w2):\n",
    "        return self.bigrams.get((w1, w2), 1) / self.unigrams.get(w1, 1)\n",
    "\n",
    "    def correct(self, sentence):\n",
    "        words = sentence.lower().split()\n",
    "        corrected_sentence = []\n",
    "        \n",
    "        for idx, word in enumerate(words):\n",
    "            candidates = generate_candidates(word)\n",
    "            if idx == 0:\n",
    "                best_word = max(candidates, key=lambda w: self.P(w))\n",
    "            else:\n",
    "                best_word = max(candidates, key=lambda w: self.P(w) * self.P_bigram(corrected_sentence[-1], w))\n",
    "            corrected_sentence.append(best_word)\n",
    "        \n",
    "        return \" \".join(corrected_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "Why Use Bigrams?\n",
    "Bigrams capture local context, meaning they help correct misspelled words based on their immediate neighbors. This is particularly useful in distinguishing between words that are often confused based on surrounding text. For example, in the phrase \"dking sport\", the bigram model helps prioritize \"doing sport\" over \"dying sport\" because \"doing sport\" is more common in the corpus.\n",
    "\n",
    "Why Not Fivegrams?\n",
    "Originally, a fivegram approach was suggested for capturing longer-range dependencies. However, fivegrams require significantly larger datasets and suffer from sparsity issues. Instead, we rely on bigrams and unigrams to balance context awareness and computational efficiency.\n",
    "\n",
    "Why Use This Approach?\n",
    "Our approach combines:\n",
    "\n",
    "Unigram frequency: Helps prioritize commonly used words.\n",
    "Bigram probabilities: Captures local context to disambiguate similar-looking words.\n",
    "Damerau-Levenshtein distance: Generates realistic spelling correction candidates by considering common types of errors (insertions, deletions, substitutions, and transpositions).\n",
    "Challenge: Context Sensitivity\n",
    "Correcting words based on both context and spelling similarity is difficult. Our solution:\n",
    "\n",
    "Uses bigrams to guide corrections based on likely word sequences.\n",
    "Uses Damerau-Levenshtein distance to suggest spelling alternatives.\n",
    "Uses probability-based selection to choose the most likely correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrected: of have to the old  true: a babe in the woods  error: a babe in the food\n",
      "corrected: the likely to the white  true: are likely to be quite  error: are unlikely to be quite\n",
      "corrected: crisis that he or the  true: crisis that led to the  error: crisis that led bol the\n",
      "corrected: a new set in a  true: has been used in a  error: has hewn used sion a\n",
      "corrected: of a very good position  true: in a very good position  error: my a very gto position\n",
      "corrected: look at the one mile  true: looked at her and smiled  error: looked tap her and smiled\n",
      "corrected: and to the door to  true: one of the four or  error: and koy the four or\n",
      "corrected: all about the decline in  true: talk about the decline of  error: talk snout the decline of\n",
      "corrected: the strength of the u.s  true: the strength of the u.s  error: shed strength of ihl u.s\n",
      "corrected: the other findings of a  true: to the findings of a  error: to the findings of yaw\n",
      "corrected: his and the children and  true: wife and two children in  error: hifi and owe childrens in\n",
      "Accuracy: 9.09%\n"
     ]
    }
   ],
   "source": [
    "def introduce_errors(sentence: str, error_prob=0.3) -> str:\n",
    "    words = sentence.split()\n",
    "    noisy_words = []\n",
    "    for word in words:\n",
    "        if random.random() < error_prob:\n",
    "            noisy_words.append(random.choice(list(generate_candidates(word)) + [word]))\n",
    "        else:\n",
    "            noisy_words.append(word)\n",
    "    return \" \".join(noisy_words)\n",
    "\n",
    "test = {}\n",
    "with open(\"fivegrams (2).txt\", \"r\", encoding=\"latin-1\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 100000 == 0:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 1:\n",
    "                freq = int(parts[0])\n",
    "                ngram = \" \".join(parts[1:])\n",
    "                test[ngram] = introduce_errors(ngram)\n",
    "\n",
    "def evaluate(corrector, test_sentences):\n",
    "    total, correct = 0, 0\n",
    "    for k, v in test_sentences.items():\n",
    "        corrected_sentence = corrector.correct(v)\n",
    "        print(f\"corrected: {corrected_sentence}  true: {k}  error: {v}\")\n",
    "        if corrected_sentence == k:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total\n",
    "\n",
    "beam_width = 5\n",
    "corrector = SpellCorrector(unigrams, bigrams, beam_width)\n",
    "\n",
    "accuracy = evaluate(corrector, test)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful resources (also included in the archive in moodle):\n",
    "\n",
    "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
    "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
